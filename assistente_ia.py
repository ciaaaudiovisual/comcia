import streamlit as st
import pandas as pd
from datetime import datetime
from database import load_data, init_supabase_client
from auth import check_permission # <-- CORRE√á√ÉO: Importa a fun√ß√£o de permiss√£o
import google.generativeai as genai
import json
import time
from io import BytesIO
import PyPDF2
import numpy as np

# ==============================================================================
# FUN√á√ïES DE BACKEND PARA INDEXA√á√ÉO E BUSCA (RAG)
# ==============================================================================

# Fun√ß√£o para ler texto de um PDF
def ler_pdf(ficheiro_bytes: BytesIO) -> str:
    leitor_pdf = PyPDF2.PdfReader(ficheiro_bytes)
    texto = ""
    for pagina in leitor_pdf.pages:
        texto += pagina.extract_text()
    return texto

# Fun√ß√£o para quebrar o texto em peda√ßos (chunks)
def dividir_em_chunks(texto: str, chunk_size=1000, overlap=100):
    chunks = []
    for i in range(0, len(texto), chunk_size - overlap):
        chunks.append(texto[i:i + chunk_size])
    return chunks

# Fun√ß√£o principal de indexa√ß√£o
def indexar_documento(nome_ficheiro: str, ficheiro_bytes: BytesIO, supabase, progress_bar):
    try:
        st.info(f"A ler o conte√∫do do ficheiro: {nome_ficheiro}...")
        texto_completo = ler_pdf(ficheiro_bytes)
        
        st.info("A dividir o documento em peda√ßos...")
        chunks = dividir_em_chunks(texto_completo)
        
        st.info(f"Encontrados {len(chunks)} peda√ßos. A criar 'impress√µes digitais' (embeddings)...")
        
        total_chunks = len(chunks)
        for i, chunk in enumerate(chunks):
            # Chama a API do Gemini para criar o embedding
            response = genai.embed_content(model='models/text-embedding-004', content=chunk)
            embedding = response['embedding']
            
            # Insere o chunk e o seu embedding no Supabase
            supabase.table('document_chunks').insert({
                'document_name': nome_ficheiro,
                'chunk_text': chunk,
                'embedding': embedding
            }).execute()

            # Respeita o limite da API para n√£o causar erros
            time.sleep(1) # Pausa de 1 segundo entre cada chamada
            progress_bar.progress((i + 1) / total_chunks, text=f"A processar peda√ßo {i+1}/{total_chunks}")

        st.success(f"Documento '{nome_ficheiro}' indexado com sucesso!")
        return True
    except Exception as e:
        st.error(f"Ocorreu um erro durante a indexa√ß√£o: {e}")
        return False

# Fun√ß√£o para buscar os chunks relevantes
def buscar_chunks_relevantes(pergunta: str, supabase, top_k=3):
    # Cria o embedding para a pergunta do utilizador
    response = genai.embed_content(model='models/text-embedding-004', content=pergunta)
    pergunta_embedding = response['embedding']
    
    # Executa a busca por similaridade de vetores no Supabase
    # Nota: O Supabase precisa de uma "RPC Function" para busca de vetores.
    # O comando para criar a fun√ß√£o no SQL Editor do Supabase √©:
    # CREATE OR REPLACE FUNCTION match_document_chunks (
    #   query_embedding vector(768),
    #   match_threshold float,
    #   match_count int
    # )
    # RETURNS TABLE (
    #   id uuid,
    #   document_name text,
    #   chunk_text text,
    #   similarity float
    # )
    # AS $$
    #   SELECT
    #     dc.id,
    #     dc.document_name,
    #     dc.chunk_text,
    #     1 - (dc.embedding <=> query_embedding) as similarity
    #   FROM document_chunks dc
    #   WHERE 1 - (dc.embedding <=> query_embedding) > match_threshold
    #   ORDER BY similarity DESC
    #   LIMIT match_count;
    # $$ LANGUAGE sql;

    resultados = supabase.rpc('match_document_chunks', {
        'query_embedding': pergunta_embedding,
        'match_threshold': 0.5, # Limiar de similaridade
        'match_count': top_k
    }).execute()
    
    return resultados.data

# Fun√ß√£o para gerar a resposta final com base no contexto
def gerar_resposta_com_contexto(pergunta: str, chunks_relevantes: list):
    contexto = "\n\n---\n\n".join([chunk['chunk_text'] for chunk in chunks_relevantes])
    
    prompt = f"""
    Voc√™ √© um assistente especialista. Responda √† pergunta do utilizador baseando-se **exclusivamente** no contexto fornecido abaixo. Se a resposta n√£o estiver no contexto, diga "A informa√ß√£o n√£o foi encontrada nos documentos dispon√≠veis."

    **Contexto:**
    {contexto}

    **Pergunta:**
    {pergunta}
    """
    model = genai.GenerativeModel('gemini-1.5-flash')
    response = model.generate_content(prompt)
    return response.text


# ==============================================================================
# P√ÅGINA PRINCIPAL DO ASSISTENTE IA (COM ABAS)
# ==============================================================================
def show_assistente_ia():
    st.title("ü§ñ Assistente de Conhecimento")
    
    supabase = init_supabase_client()
    try:
        api_key = st.secrets["google_ai"]["api_key"]
        genai.configure(api_key=api_key)
    except Exception as e:
        st.error(f"Erro ao configurar a API do Gemini. Verifique os segredos: {e}")
        return

    # Cria as abas
    tab_consulta, tab_gestao = st.tabs(["Consultar Documentos", "Gerir Documentos"])

    # --- ABA DE CONSULTA PARA TODOS OS UTILIZADORES ---
    with tab_consulta:
        st.subheader("Fa√ßa uma pergunta sobre os regulamentos")
        
        if "chat_messages" not in st.session_state:
            st.session_state.chat_messages = [{"role": "assistant", "content": "Ol√°! Em que posso ajudar com base nos documentos?"}]

        for message in st.session_state.chat_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        prompt_texto = st.chat_input("Qual √© a sua d√∫vida?")
        if prompt_texto:
            st.session_state.chat_messages.append({"role": "user", "content": prompt_texto})
            with st.chat_message("user"):
                st.markdown(prompt_texto)
            
            with st.chat_message("assistant"):
                with st.spinner("A consultar a base de conhecimento..."):
                    chunks = buscar_chunks_relevantes(prompt_texto, supabase)
                    if not chunks:
                        resposta = "N√£o encontrei informa√ß√µes relevantes nos documentos para responder √† sua pergunta."
                    else:
                        resposta = gerar_resposta_com_contexto(prompt_texto, chunks)
                st.markdown(resposta)
            
            st.session_state.chat_messages.append({"role": "assistant", "content": resposta})

    # --- ABA DE GEST√ÉO APENAS PARA ADMINS ---
    with tab_gestao:
        st.subheader("Gest√£o da Base de Conhecimento")
        if check_permission('pode_gerenciar_usuarios'): # Usando uma permiss√£o de admin existente como exemplo
            
            # Sec√ß√£o de Upload e Indexa√ß√£o
            st.markdown("#### Carregar e Indexar Novo Documento")
            uploaded_file = st.file_uploader("Escolha um ficheiro PDF", type="pdf")
            if uploaded_file is not None:
                if st.button(f"Indexar Ficheiro: {uploaded_file.name}"):
                    progress_bar = st.progress(0, text="A iniciar a indexa√ß√£o...")
                    indexar_documento(uploaded_file.name, BytesIO(uploaded_file.getvalue()), supabase, progress_bar)
                    st.success("Processo de indexa√ß√£o conclu√≠do!")

            st.divider()

            # Sec√ß√£o para ver documentos j√° indexados
            st.markdown("#### Documentos na Mem√≥ria da IA")
            try:
                documentos_db = supabase.table('document_chunks').select('document_name').execute()
                if documentos_db.data:
                    nomes_unicos = sorted(list(set(item['document_name'] for item in documentos_db.data)))
                    for nome in nomes_unicos:
                        st.write(f"üìÑ {nome}")
                else:
                    st.info("Nenhum documento foi indexado ainda.")
            except Exception as e:
                st.warning(f"N√£o foi poss√≠vel listar os documentos. A tabela 'document_chunks' existe? Erro: {e}")

        else:
            st.error("Apenas administradores podem aceder a esta sec√ß√£o.")
